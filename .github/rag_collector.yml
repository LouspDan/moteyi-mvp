# .github/workflows/rag_collector.yml
name: RAG Document Collector & Validator

on:
  schedule:
    # Ex√©cution hebdomadaire le lundi √† 2h00 UTC
    - cron: '0 2 * * 1'
  workflow_dispatch:
    inputs:
      validate_only:
        description: 'Valider uniquement les documents existants'
        required: false
        default: 'false'
      specific_subject:
        description: 'Collecter une mati√®re sp√©cifique (maths, francais, svt, etc.)'
        required: false
        default: 'all'

env:
  PYTHON_VERSION: '3.11'
  RAG_DATA_PATH: data/rag_seed
  CATALOG_FILE: data/rag_seed/rag_seed_catalog.csv

jobs:
  collect-documents:
    name: üìö Collecte des documents
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.validate_only != 'true' }}
    
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: üêç Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: üì¶ Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r ${{ env.RAG_DATA_PATH }}/scripts/requirements.txt
          
      - name: üîç Check for new documents
        id: check_new
        run: |
          cd ${{ env.RAG_DATA_PATH }}/scripts
          python -c "
import requests
from bs4 import BeautifulSoup
import csv
import os

# Charger le catalogue existant
existing_urls = set()
if os.path.exists('../rag_seed_catalog.csv'):
    with open('../rag_seed_catalog.csv', 'r') as f:
        reader = csv.DictReader(f)
        existing_urls = {row['source_url'] for row in reader if row.get('source_url')}

# V√©rifier les nouveaux documents
response = requests.get('https://edu-nc.gouv.cd/programmes-scolaires')
soup = BeautifulSoup(response.content, 'html.parser')
pdf_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.pdf')]

new_docs = [url for url in pdf_links if url not in existing_urls]
print(f'Nouveaux documents trouv√©s: {len(new_docs)}')

if new_docs:
    print('::set-output name=has_new::true')
    print(f'::set-output name=count::{len(new_docs)}')
else:
    print('::set-output name=has_new::false')
"
          
      - name: üöÄ Run document collector
        if: steps.check_new.outputs.has_new == 'true'
        run: |
          cd ${{ env.RAG_DATA_PATH }}/scripts
          
          if [ "${{ github.event.inputs.specific_subject }}" != "all" ]; then
            echo "Collecte sp√©cifique: ${{ github.event.inputs.specific_subject }}"
            python scraper_edu_rdc.py --subject "${{ github.event.inputs.specific_subject }}"
          else
            echo "Collecte compl√®te des documents"
            python scraper_edu_rdc.py
          fi
          
      - name: üìä Generate collection report
        if: steps.check_new.outputs.has_new == 'true'
        run: |
          cd ${{ env.RAG_DATA_PATH }}
          python -c "
import csv
import json

with open('rag_seed_catalog.csv', 'r') as f:
    reader = csv.DictReader(f)
    docs = list(reader)

stats = {
    'total': len(docs),
    'par_matiere': {},
    'par_langue': {},
    'par_niveau': {}
}

for doc in docs:
    # Par mati√®re
    matiere = doc.get('matiere', 'autre')
    stats['par_matiere'][matiere] = stats['par_matiere'].get(matiere, 0) + 1
    
    # Par langue
    langue = doc.get('langue', 'autre')
    stats['par_langue'][langue] = stats['par_langue'].get(langue, 0) + 1
    
    # Par niveau
    niveau = doc.get('grade_level', 'autre')
    stats['par_niveau'][niveau] = stats['par_niveau'].get(niveau, 0) + 1

print(json.dumps(stats, indent=2))

# Sauvegarder le rapport
with open('collection_report.json', 'w') as f:
    json.dump(stats, f, indent=2)
"
          
      - name: üíæ Upload artifacts
        if: steps.check_new.outputs.has_new == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: rag-documents-${{ github.run_number }}
          path: |
            ${{ env.RAG_DATA_PATH }}/**/*.pdf
            ${{ env.RAG_DATA_PATH }}/collection_report.json
          retention-days: 30

  validate-documents:
    name: ‚úÖ Validation des documents
    runs-on: ubuntu-latest
    needs: [collect-documents]
    if: always()
    
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4
        
      - name: üêç Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: üì¶ Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r ${{ env.RAG_DATA_PATH }}/scripts/requirements.txt
          
      - name: üîç Run validation
        id: validation
        run: |
          cd ${{ env.RAG_DATA_PATH }}/scripts
          
          # Ex√©cuter la validation
          python validator.py > validation_output.txt 2>&1
          
          # Analyser les r√©sultats
          python -c "
import re

with open('validation_output.txt', 'r') as f:
    output = f.read()

# Extraire les statistiques
ok_match = re.search(r'‚úÖ OK: (\d+)', output)
attention_match = re.search(r'‚ö†Ô∏è  Attention: (\d+)', output)
error_match = re.search(r'‚ùå Erreur: (\d+)', output)

ok_count = int(ok_match.group(1)) if ok_match else 0
attention_count = int(attention_match.group(1)) if attention_match else 0
error_count = int(error_match.group(1)) if error_match else 0

print(f'::set-output name=ok_count::{ok_count}')
print(f'::set-output name=attention_count::{attention_count}')
print(f'::set-output name=error_count::{error_count}')

# D√©terminer le statut
if error_count > 0:
    print('::set-output name=status::error')
elif attention_count > 5:
    print('::set-output name=status::warning')
else:
    print('::set-output name=status::success')
"
          
      - name: üìù Create validation report
        run: |
          cd ${{ env.RAG_DATA_PATH }}
          
          cat << EOF > validation_report.md
          # üìä Rapport de Validation RAG
          
          ## Statistiques
          - ‚úÖ Documents OK: ${{ steps.validation.outputs.ok_count }}
          - ‚ö†Ô∏è Attention requise: ${{ steps.validation.outputs.attention_count }}
          - ‚ùå Erreurs: ${{ steps.validation.outputs.error_count }}
          
          ## D√©tails
          \`\`\`
          $(cat scripts/validation_output.txt)
          \`\`\`
          
          ## Actions Recommand√©es
          EOF
          
          if [ "${{ steps.validation.outputs.error_count }}" -gt "0" ]; then
            echo "- üî¥ Corriger les erreurs critiques avant d√©ploiement" >> validation_report.md
          fi
          
          if [ "${{ steps.validation.outputs.attention_count }}" -gt "0" ]; then
            echo "- üü° V√©rifier les documents signal√©s" >> validation_report.md
          fi
          
          echo "- üü¢ Documents valid√©s pr√™ts pour l'ingestion RAG" >> validation_report.md
          
      - name: üí¨ Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('${{ env.RAG_DATA_PATH }}/validation_report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

  update-catalog:
    name: üìù Mise √† jour du catalogue
    runs-on: ubuntu-latest
    needs: [validate-documents]
    if: needs.validate-documents.outputs.status != 'error'
    
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: üîÑ Update catalog statistics
        run: |
          cd ${{ env.RAG_DATA_PATH }}
          
          # G√©n√©rer les statistiques README
          python -c "
import csv
from datetime import datetime

with open('rag_seed_catalog.csv', 'r') as f:
    reader = csv.DictReader(f)
    docs = list(reader)

# Calculer les stats
total = len(docs)
validated = sum(1 for d in docs if d.get('validated') == 'true')
by_lang = {}
by_subject = {}

for doc in docs:
    lang = doc.get('langue', 'autre')
    by_lang[lang] = by_lang.get(lang, 0) + 1
    
    subject = doc.get('matiere', 'autre')
    by_subject[subject] = by_subject.get(subject, 0) + 1

# Cr√©er le README stats
readme_content = f'''# üìä √âtat de la Base RAG - Moteyi AI

*Derni√®re mise √† jour : {datetime.now().strftime('%Y-%m-%d %H:%M')}*

## Statistiques Globales
- **Total documents** : {total}
- **Documents valid√©s** : {validated} ({validated/total*100:.1f}%)
- **Taux de couverture** : {min(total/100*100, 100):.1f}%

## Par Langue
'''

for lang, count in sorted(by_lang.items()):
    readme_content += f'- {lang.capitalize()} : {count} documents\\n'

readme_content += '\\n## Par Mati√®re\\n'
for subject, count in sorted(by_subject.items()):
    readme_content += f'- {subject.capitalize()} : {count} documents\\n'

with open('RAG_STATUS.md', 'w') as f:
    f.write(readme_content)
"
          
      - name: üì§ Commit and push changes
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add ${{ env.RAG_DATA_PATH }}/
          
          if git diff --staged --quiet; then
            echo "Pas de changements √† commiter"
          else
            git commit -m "feat(rag): mise √† jour catalogue documents [skip ci]
            
            - Documents ajout√©s/mis √† jour
            - Validation automatique ex√©cut√©e
            - Catalogue CSV actualis√©"
            
            git push origin ${{ github.ref }}
          fi

  notify-results:
    name: üì¢ Notification des r√©sultats
    runs-on: ubuntu-latest
    needs: [collect-documents, validate-documents]
    if: always()
    
    steps:
      - name: üìä Prepare summary
        run: |
          echo "## üìö R√©sultats de la Collecte RAG" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.collect-documents.outputs.has_new }}" == "true" ]; then
            echo "‚úÖ **Nouveaux documents collect√©s** : ${{ needs.collect-documents.outputs.count }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ÑπÔ∏è Aucun nouveau document d√©tect√©" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Validation" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ OK : ${{ needs.validate-documents.outputs.ok_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- ‚ö†Ô∏è Attention : ${{ needs.validate-documents.outputs.attention_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- ‚ùå Erreurs : ${{ needs.validate-documents.outputs.error_count }}" >> $GITHUB_STEP_SUMMARY
          
      - name: üîî Send Slack notification
        if: env.SLACK_WEBHOOK_URL != ''
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          STATUS_EMOJI="‚úÖ"
          STATUS_TEXT="succ√®s"
          
          if [ "${{ needs.validate-documents.outputs.status }}" == "error" ]; then
            STATUS_EMOJI="‚ùå"
            STATUS_TEXT="erreurs d√©tect√©es"
          elif [ "${{ needs.validate-documents.outputs.status }}" == "warning" ]; then
            STATUS_EMOJI="‚ö†Ô∏è"
            STATUS_TEXT="attention requise"
          fi
          
          curl -X POST $SLACK_WEBHOOK_URL \
            -H 'Content-Type: application/json' \
            -d "{
              \"text\": \"$STATUS_EMOJI Collecte RAG - $STATUS_TEXT\",
              \"blocks\": [
                {
                  \"type\": \"section\",
                  \"text\": {
                    \"type\": \"mrkdwn\",
                    \"text\": \"*Collecte RAG Moteyi*\\n‚Ä¢ Statut: $STATUS_TEXT\\n‚Ä¢ Documents OK: ${{ needs.validate-documents.outputs.ok_count }}\\n‚Ä¢ Voir: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|D√©tails>\"
                  }
                }
              ]
            }"

  # Job optionnel pour l'ingestion dans le syst√®me RAG
  ingest-to-rag:
    name: üîÑ Ingestion RAG
    runs-on: ubuntu-latest
    needs: [update-catalog]
    if: needs.validate-documents.outputs.status == 'success'
    
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4
        
      - name: üêç Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: üì¶ Install RAG dependencies
        run: |
          pip install langchain chromadb openai tiktoken
          
      - name: üß† Ingest documents to vector store
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          echo "‚è≥ Ingestion des documents dans le vector store..."
          
          # Script d'ingestion (√† adapter selon votre stack RAG)
          python -c "
import os
import csv
from pathlib import Path

# Placeholder pour l'ingestion r√©elle
# √Ä remplacer par votre logique d'ingestion RAG

catalog_path = '${{ env.CATALOG_FILE }}'
with open(catalog_path, 'r') as f:
    reader = csv.DictReader(f)
    docs = [d for d in reader if d.get('validated') == 'true']

print(f'üìö {len(docs)} documents pr√™ts pour l\'ingestion')

# TODO: Impl√©menter l'ingestion r√©elle
# - Charger les PDFs
# - Extraire le texte
# - Chunking
# - Embedding
# - Stockage dans ChromaDB/Pinecone/etc.

print('‚úÖ Ingestion simul√©e avec succ√®s')
"
          
      - name: üéâ Final success message
        run: |
          echo "‚úÖ Pipeline RAG compl√©t√© avec succ√®s!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Les documents sont maintenant disponibles dans le syst√®me RAG de Moteyi AI." >> $GITHUB_STEP_SUMMARY